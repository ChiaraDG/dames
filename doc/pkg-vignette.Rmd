---
title: "Two-Phase ODS with Marginalized Models"
output: 
  rmarkdown::html_vignette:
    toc: yes
    toc_depth: 3
vignette: >
  %\VignetteIndexEntry{Two-Phase ODS with Marginalized Models}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

The vignette describe the `ods2phMM` R package, and provide an example on how to set up a design and analysis for a two-phase outcome dependent study (ODS) with binary longitudinal data. The package aims to facilitate the use of two-phase ODS design by providing: (1) a set of functions to select which individual to include in the study, and (2) a set of function to estimate the parameters of interest. Notes and background information of the designs and methods presented in this vignette can be found at the end of the document.

# Basic Example

## Installation

The `ods2phMM` can be installed

INSTRUCTION

Once the package is installed, you can load to the current environment as:

```{r setup}
library(ods2phMM)
```

Note that the package `ods2phMM` depends on the R package `MMLB`, so make sure to have `MMLB` installed. More information about the `MMLB` package and how to install it can be found [here](https://github.com/mercaldo/MMLB).

## The Data

The dataset used in this vignette is stored in the `ods2phMM` under the name `exampledat`. The data were randomly generated from the model:

$$logit(\mu_{ij}^m) = -2 + X_{i} + 0.15T_{ij} + 0.25Z_i + 0.15 X_i T_{ij}$$

$$logit(\mu_{ij}^c) = \Delta_{ij} + 2 Y_{i(j-1)} + 0 U_i$$

where $i = 1, ..., 1,000$ indicates subject $i$, $X_i$ and $Z_i$ are binary time-fixed covariates and $T_{ij}$ is a time-varying covariates indicating visit time (each subject $i$ is observed three to six times).

```{r dataload}
str(exampledat)
```

For the remaning part of this vignette we are going to assume that:

* $X_i$ is an expensive time-fixed exposure that needs to be collected using a two-phase ODS design.

* $Z_i$ is a inexpensive time-fixed covariate that is available for the 1,000 subjects in the dataset.

**Note that the function in the package support data.frame class only. Tibbles are not currently supported.**

## The Designs

`ods2phMM` allows us to perform two broad classes of two-phase outcome dependent sampling (ODS) designs: the none-some-all (NSA) designs and the residual-based designs. Broadly, the NSA design identifies informative individuals based on a summary of their outcome only, while residual-based designs use information on outcome and available covariates to identify the most informative individuals. Detailed information on each design can be found at the end of this vignette. In this section, we are going to see how to use the functions in `ods2phMM` to perform NSA and residual based designs given a dataset.

* The function `nsa_design()`, allow us to perform any NSA design. The function requires the following input:

  - `Y`: the name of the column storing the outcome value in the dataset
  
  - `id`: the name of the column storing the subject indicator in the dataset
  
  - `n.sample`: a vector with three elements indicating how many subjects we want to sample from the none, the some and the all strata respectively
  
  - `data`: the name of the dataset
  
  The function will then output a list with three elements:
  
  - `sampled.id`: list of subjects id to sample
  
  - `sample.probs`: vector of sampling probabilities from the none, some, all strata to be used in the analysis 
  
  - `n.stratum`: vector specifying the number of subjects in the none, some, all strata respectively
  
  Below is an example of usage of the `nsa_design()` function. Here we want to sample 85 people from the none stratum, 100 people from the some stratum and 15 people. Note, since independent Bernoulli sampling is performed, it is not unusual to observe a sample size not equal to 200.
  
```{r nsadesign}
nsa_design(Y = "Y", id = "id", 
           n.sample = c(85, 100, 15), data = exampledat)
```

* The function `res_design()`, allow us to perform any residual-based design. The function requires the following input:

  - `Y`: the name of the column storing the outcome value in the dataset
  
  - `id`: the name of the column storing the subject indicator in the dataset
  
  - `mean.model`: formula for the marginal mean model in which a binary variable is regressed on a set of covariates. The formula for the mean model should be the same as the formula of the model we are interested in where we omit all the terms that relates to the expensive exposure. 
  
  - `t.model`: formula for the transition part in the dependence model. The formula for the transition part in the dependence model should be the same as the formula of the model we are interested in.
  
  - `lv.model`: formula for the latent part in the dependence model. he formula for the latent part in the dependence model should be the same as the formula of the model we are interested in.
  
  - `n.sample`: number of subjects to be sampled using the residual-based design
  
  - `statistic`: name of the statistic to be used for the residual-based design. Currently, the statistics supported are the mean of the residuals, the absolute value of the mean of the residuals and the the variance of the residuals.
  
   - `data`: the name of the dataset
  
  The function will then output a list with one element:
  
  - `sampled.id`: list of subjects id to sample
  
   Below is an example of usage of the `nres_design()` function. Here we want to sample 200 people with the highest absolute value of the residuals:
  
```{r resdesign}
res_design(Y = "Y", id = "id", 
           mean.model = Y ~ Z + time, t.model = ~1, lv.model = NULL,
           n.sample = 200, statistic = "abs.mean", data = exampledat)
```

  

## The Inference Methods 

Inference methods can be divided into two main classes: methods that only use subjects with complete data on outcome, expensive exposure and inexpensive covariates (complete case analyses) and methods that combine subjects with complete data on outcome, expensive exposure and inexpensive covariates and those partial data with outcome and inexpensive covariates (full cohort analyses).

The `ods2phMM` package allows the users to perform two complete-case analyses and three full-cohort analyses. The two complete-case analyses are:

* _Ascertainment Corrected Maximum Likelihood._ The method can be used regardless of the type of expensive exposure (i.e., nominal or continuous). However, note that ascertainment corrected maximum likelihood is currently implemented for the NSA design only, and the code does not support residual-based designs.

* _Weighted Maximum Likelihood_

The three full-cohort analyses are:

* _Multiple Imputation._ The method currently support binary expensive covariates only. 

* _EM Algorithm_ The method currently support binary and/or categorical expensive covariates only.The EM algorithm can be used regardless of the design.

* _Sieve Maximum Likelihood_ The method can be used regardless of the type of expensive exposure (i.e., nominal or continuous) and/or design.

Details on each methods, together with references, are presented at the end of the document. In this section, we are going to show how each method can be applied to the `exampledat` dataset.

### Complete Case Analyses 

* ACML

* Weighted Likelihood

### Full Cohort Analysis

This will require 3 functions `smle_mm()`, `mi_mm()` and `em_mm()`

* Multiple imputation

* EM algorithm

* SMLE

## Summarise the results

_Not an integral part of the tutorial but creating `summary()` functions will be important for a potential package_

# Overview of the package

OVERVIEW: DESIGN + INFERENCE

In total, there are xx functions and xx datasets in the package that available to the user:

| Name                  | Function and Outcome Description                                                             |
|-----------------------|----------------------------------------------------------------------------------------------|
| $\texttt{nsa_design}$ | Perform the NSA design. Returns a list with three elements: 1) vector of IDs to be sampled for exposure ascertainment, 2) vector of sampling probabilities from the none, some and all strata respectively, and 3) vector of sample sizes from the none some and all strata respectively  |
| $\texttt{res_design}$ | Perform the residual-based design. Currently, sampling based on the mean, the absolute value of the mean or the variance of the residuals are implemented. Returns the vector of IDs to be sampled for exposure ascertainment. |
| $\texttt{acml_mm}$    | Perform ascertainment corrected maximum likelihood for the NSA design. Returns a list with estimated parameters and the corresponding covariance matrix. |



# Notes on Methodology

## Marginalized Transition and Latent Variable Model

Let $N$ be the total number of subjects, $\boldsymbol{Y}_{i}$ be the $n_i-$vector of binary responses for subject $i$, $\boldsymbol{X}_i$ be the $n_i \times p$ matrix of covariates and $U_i \sim N(0, 1)$. The marginalized transition and latent variable (mTLV) model described in [[1]](https://pubmed.ncbi.nlm.nih.gov/17688485/) can be defined by two equations:

$$logit\left(\mu_{ij}^m\right) = \beta_0 + \boldsymbol{\beta}^T\boldsymbol{X}_i$$
$$logit\left(\mu_{ij}^c\right) = \Delta_{ij}(\boldsymbol{X}_i) + \gamma(\boldsymbol{X}_i)Y_{i(j-1)} + \sigma U_i$$
The first equation, describing the marginal mean model, allow us to understand the relationship between the outcome $\boldsymbol{Y}_{i}$ and the covariates $\boldsymbol{X}_i$. The second equation, describing the conditional mean model, allow us to understand the relationship between the outcome $\boldsymbol{Y}_{i}$ measured over time for each subject $i$. Specifically, the conditional mean model includes a short-term transition component, $\gamma(\boldsymbol{X}_i)Y_{i(j-1)}$, and a random intercept term, $Z_i$, describing a longer term dependence. $\Delta_{ij}(\boldsymbol{X}_i)$ is a function of $\mu_{ij}^m$ and  $\mu_{ij}^c$ such that the marginal and the conditional models are cohesive.

## The Designs

<div style= "float:right;position: relative; padding:30px;">
![](phase1and2.png){width=20%,height=20%}
</div>

The main idea behind the two-phase ODS designs is to increase precision of the estimated coefficients by maximizing observed response variability. 

Briefly, in a two-phase ODS design, the longitudinal outcome and a series of inexpensive covariates are observed for all subjects in **phase one**. Then, a subset of the subjects is chosen for a **phase two** based on their observed outcome and inexpensive covariates. Only subjects selected for phase two will have the expensive exposure measured.

The choice of design should depend on the available data as well as the inferential target. In this section, we present two class of two-phase ODS designs that can be implemented when we have a binary longitudinal outcome.


### The NSA Design

In [[2]](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2733177/) and [[3]](https://pubmed.ncbi.nlm.nih.gov/21457191/), Schildcrout et al introduced the NSA design which identifies informative individuals based on a summary of their outcome vector $\boldsymbol{Y}_{i} = (Y_{i1}, ..., Y_{in_i})$. Specifically, for each subject $i$, the NSA design computes $S_i = \sum_{j = 1}^{n_i}Y_{ij}$ and classifies them in one of the three strata:

* **None Stratum**: people who never experienced the outcome $(S_i = 0)$

* **Some Stratum**: people who exhibit response variation $(0 < S_i < n_i)$

* **All Stratum**: people who always experience the outcome $(S_i = n_i)$

The graphical representation below helps us identify how the outcome change over time for subjects in the none, some and all strata

```{r, message=FALSE, echo=FALSE, fig.width=8, fig.height=5,fig.retina=3}
library(tidyverse)
library(viridis)

exampledat <- exampledat %>% group_by(id) %>%
  mutate(mY = mean(Y), stratum = ifelse(mY == 0, "None",
                                        ifelse(mY == 1, "All", "Some")))
exampledat$stratum <- factor(exampledat$stratum, levels = c("None", "Some", "All"),
                             ordered = TRUE)
ggplot(exampledat, aes(x = time, y = id, fill = factor(Y))) + geom_tile() + 
  scale_fill_viridis(discrete = TRUE, begin = 0.1, end = 0.9, 
                     name = "Outcome Level") + 
  facet_wrap(stratum~.) +
  theme_bw() + theme(legend.position = "bottom")
```

Increased efficiency of the estimated model's parameters can be observed by sampling from each of the three strata with different probabilities. Specifically:

* if interest lies in making inference on coefficients associated with time-varying covariates, then we should increase within-subjects' variability. A possible way to do so is to oversample subjects in the same stratum.

* If interest lies in making inference on coefficients associated with time-fixed covariates, then we should increase between-subjects' variability. A possible way to do so is to oversample subjects in the none and the all stratum.  

* If interest lies in making inference on coefficients or associated with both time-varying and time-fixed covariates, then one should increase both between-subjects and within-subjects' variability. A possible way to do so is to sample from the none, the some and the all strata.

### The Residual-Based Design

In **?**, Di Gravio et al introduced the residual-based design which identifies informative individuals based on a summary of their outcome vector $\boldsymbol{Y}_{i} = (Y_{i1}, ..., Y_{in_i})$ and inexpensive covariates $\boldsymbol{Z}$. Specifically, residual-based designs are based on the following steps:

1. Fit a mTLV model where we exclude the expensive covariates:

$$logit(\mu_{ij}^m) = \beta_0^{*} + \beta_t^{*}T_{ij} + \beta_z^{*}Z_i$$

$$logit(\mu_{ij}^c) = \Delta_{ij} + \gamma^{*} Y_{i(j-1)} + \sigma U_i$$

2. Compute $\hat{\mu}_{ij}^m = \frac{exp \left(\hat{\beta}_0^{*} + \hat{\beta}_t^{*}T_{ij} + \hat{\beta}_z^{*}Z_i \right)}{1 + exp \left(\hat{\beta}_0^{*} + \hat{\beta}_t^{*}T_{ij} + \hat{\beta}_z^{*}Z_i \right)}$

3. Compute $\hat{\epsilon}_{ij} = Y_{ij} - \hat{\mu}_{ij}^m$ for each subject $i$ at time $j$

4. For each subject $i$ compute a summary (mean, absolute value of the mean or variance) of $\epsilon_{ij}$. 

Increased efficiency of the estimated model’s parameters can be observed by sampling based on a summary of $\epsilon_{ij}$:

* If interest lies in making inference on coefficients associated with time-varying covariates, then we should increase within-subjects' variability. This can be achieved by sampling those subjects $j$ with the highest value of the within-subjects' variance of $\epsilon_{ij}$.

* if interested lies in making inference on coefficients associated with time-fixed covariates, then we should increase between-subjects' variability. This can be achieved by sampling those subjects with the highest absolute value of the mean of $\epsilon_{ij}$ or those with the highest and lowest value of the mean of $\epsilon_{ij}$.

Note that the design based on the absolute value of the mean of $\epsilon_{ij}$ can help identify those with the extreme values of the mean in situations where the distribution of $\epsilon_{ij}$ is not symmetric.

### Relationship between NSA and Residual-Based Design

Differently from the NSA designs, residual-based designs use information on the outcome and inexpensive covariates in the sampling scheme. Ideally, adding information on the expensive covariates can increase the precision of:

1) estimated coefficients associated with **time-fixed covariates**, and

2) estimated coefficients associated with **time-varying covariates** in cases where there is high heterogeneity in the some stratum (for instance, due to a high number of observations per subjects), and we are only able to sample a small percentage of people for phase two. 

The table below show in which of the none, some and all strata subjects sampled using residual-based designs belongs to.

| Residual-Based Design                    | None | Some | All |
|------------------------------------------|:----:|:----:|:---:|
| Variance of Residuals                    |  0   | 200  | 0   |
| Mean of Residuals                        | 100  | 81   | 19  |
| Absolute Value of Mean of the Residuals  |  85  | 96   | 19  |

Thus, the design based on the variance of the residuals only sample those in the some stratum; the designs based on the mean (and the absolute value of the mean) of the residuals sample all subjects from the all stratum. 


## The Estimation Procedures

Estimation procedures can be divided into two main classes: methods that only use subjects with complete data on outcome, expensive exposure and inexpensive covariates (complete-case analyses) and methods that combine subjects with complete data on outcome, expensive exposure and inexpensive covariates and those partial data with outcome and inexpensive covariates (full-cohort analyses). 

The `ods2phMM` package includes two complete-case analyses and three full-cohort analyses. The two complete-case analyses are the ascertainment corrected maximum likelihood (ACML) described by Schildcrout et al in [[2]](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2733177/), and the weighted maximum likelihood described by Lawless et al in **?**. The three full cohort analyses are the multiple imputation procedure and the EM algorith described by **?** in **?**, and the sieve maximum likelihood estimator described by Tao et al and Di Gravio et al in **?**.

### Complete-Case Analyses

#### Ascertainment Corrected Maximum Likelihood (ACML)

#### Weighted Maximum Likelihood 

### Full-Cohort Analyses

#### Multiple Imputation

#### EM Algorithm

#### Sieve Maximum Likelihood Estimator (SMLE)

# Reference

[[1]](https://pubmed.ncbi.nlm.nih.gov/17688485/) Schildcrout, JS. and Heagerty, PJ. (2007). Marginalized models for moderate to long series of longitudinal binary response data. Biometrics 63(2), 322–331.

[[2]](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2733177/) Schildcrout JS, and Heagerty PJ (2008). On outcome-dependent sampling designs for longitudinal binary response data with time-varying covariates. Biostatistics, 9(4), 735-749.

[[3]](https://pubmed.ncbi.nlm.nih.gov/21457191/) Schildcrout JS, and Heagerty PJ (2011). Outcome dependent sampling from existing cohorts with longitudinal binary response data: study planning and analysis. Biometrics, 67(4), 1583-1593.
